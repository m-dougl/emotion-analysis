{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f892598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dougl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\dougl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "ERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\n",
      "ERROR: No matching distribution found for unicodedata\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "! pip install unicodedata\n",
    "from unicodedata import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ca924bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unicodedata import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "def open_dataset(dataset_name, dataset_type):\n",
    "    if dataset_type == 'excel' or 'xlsx':\n",
    "        df = pd.read_excel(str(dataset_name))\n",
    "        return df\n",
    "    elif dataset_type == 'csv':\n",
    "        df = pd.read_excel(str(dataset_name))\n",
    "        return df\n",
    "\n",
    "def remove_characters(txt):\n",
    "    sc = [c for c in txt.lower() if c not in string.punctuation]\n",
    "    sc_ = ''.join(sc)\n",
    "    return sc_\n",
    "\n",
    "def remove_accents(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def tokenize(txt):\n",
    "    return RegexpTokenizer('\\w+').tokenize(txt)\n",
    "\n",
    "def untokenize(txt):\n",
    "    return (' ').join(i for i in txt if len(i)>1)\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    return ' '.join([w for w in txt.split() if w.lower() not in nltk.corpus.stopwords.words('portuguese')])\n",
    "\n",
    "def stemming(txt):\n",
    "    return ' '.join([nltk.stem.RSLPStemmer().stem(i) for i in txt.split()])\n",
    "\n",
    "def simple_train(classifier_name, X, y):\n",
    "    if classifier_name == 'Naive Bayes' or 'NB':\n",
    "        clf = MultinomialNB().fit(X, y)\n",
    "        return clf\n",
    "    elif classifier_name == 'Support Vector Machine' or 'SVM':\n",
    "        clf = SVC(gamma=1, C=1, kernel = 'rbf').fit(X, y)\n",
    "        return clf\n",
    "    elif classifier_name == 'K Nearest Neighbors' or 'KNN':\n",
    "        clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "        return clf\n",
    "\n",
    "def cv_train(classifier_name, X, y, n_fold=5):\n",
    "    if classifier_name == 'Naive Bayes' or 'NB':\n",
    "        clf = GridSearchCV(estimator  = MultinomialNB(),\n",
    "                           param_grid = {'alpha': [0.001,0.01,0.1,1,10,100,1000],\n",
    "                                         'fit_prior':[True, False]},\n",
    "                           cv         = n_fold).fit(X, y)\n",
    "        return clf\n",
    "    \n",
    "    elif classifier_name == 'Support Vector Machine' or 'SVM':    \n",
    "        clf = GridSearchCV(estimator  = SVC(),\n",
    "                           param_grid = {'C': [0.001,0.01,0.1,1,10,100,1000],\n",
    "                                         'kernel': ['linear','poly','rbf','sigmoid'],\n",
    "                                         'gamma': [0.001,0.01,0.1,1,10,100]},\n",
    "                           cv         = n_fold).fit(X, y)\n",
    "        return clf\n",
    "    \n",
    "    elif classifier_name == 'K Nearest Neighbors' or 'KNN':\n",
    "        clf = GridSearchCV(estimator  = KNeighborsClassifier(),\n",
    "                           param_grid = {'n_neighbors': [2,3,4,5,6],\n",
    "                                         'algoritm':['auto','ball_tree','kd_tree','brute']},\n",
    "                           cv         = n_fold).fit(X, y)\n",
    "        return clf\n",
    "\n",
    "def vectorizer(X, vec_type):\n",
    "    if vec_type == 'countvectorizer':\n",
    "        CountVec = CountVectorizer()\n",
    "        numeric_matrix = CountVec.fit_transform(X)\n",
    "        return numeric_matrix\n",
    "    \n",
    "    elif vec_type == 'tfidf-vectorizer' or 'tfidf':\n",
    "        Tfidf = TfidfVectorizer()\n",
    "        numeric_matrix = Tfidf.fit_transform(X)\n",
    "        return numeric_matrix\n",
    "    \n",
    "def dataset_split(X, y, train_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       train_size   = train_size,\n",
    "                                                       strify       = y,\n",
    "                                                       random_state = 0,\n",
    "                                                       shuffle      = True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def confusion_matrix_analysis(y_test, list_predict):\n",
    "    cm = []\n",
    "    for i in range(len(list_predict)):\n",
    "        cm.append(confusion_matrix(y_test, list_predict[i]))\n",
    "    axis = sns.heatmap(cm/np.sum(cm),\n",
    "                       annot = True,\n",
    "                       fmt   = '.2%',\n",
    "                       cmap  = 'Blues')\n",
    "    return cm\n",
    "\n",
    "def confusion_matrix_plot(models, X_test, y_test):\n",
    "    fig, axis = plt.subplots(nrows   = 2,\n",
    "                             ncols   = 2,\n",
    "                             figsize = (15,10))\n",
    "    for clf, ax in zip(models, axis.flatten()):\n",
    "        plot_confusion_matrix(clf, X_test, y_test,\n",
    "                              ax = axis, cmap= 'Blues',\n",
    "                              display_labels=y_test)\n",
    "        axis.title.set_text(type(clf).__name__)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def confusion_matrix_plot(y_test, list_predict):\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm_list = []\n",
    "        for i in range(0, len(list_predict)):\n",
    "            cm_list.append(confusion_matrix(y_test, list_predict[i]))\n",
    "        \n",
    "        return cm_list\n",
    "\n",
    "    def accuracy(y_test, list_predict):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = []\n",
    "        for i in range(0, len(list_predict)):\n",
    "            accuracy.append(int(accuracy_score(y_test, list_predict[i])100))\n",
    "        return accuracy\n",
    "\n",
    "    def fscore(y_test, list_predict):\n",
    "        from sklearn.metrics import f1_score\n",
    "        fscore = []\n",
    "        for i in range(0, len(list_predict)):\n",
    "            fscore.append(int(f1_score(y_test, list_predict[i], average = 'macro')100))\n",
    "        return fscore\n",
    "\n",
    "    def recall(y_test, list_predict):\n",
    "        from sklearn.metrics import recall_score\n",
    "        recall = []\n",
    "        for i in range(0, len(list_predict)):\n",
    "            recall.append(int(recall_score(y_test, list_predict[i], average = 'macro')100))\n",
    "        return recall\n",
    "\n",
    "    def precision(y_test, list_predict):\n",
    "        from sklearn.metrics import precision_score\n",
    "        precision = []\n",
    "        for i in range(0, len(list_predict)):\n",
    "            precision.append(int(precision_score(y_test, list_predict[i], average = 'macro')100))\n",
    "        return precision\n",
    "\n",
    "    def get_proba(X_test, list_model):\n",
    "        proba_values = []\n",
    "        for i in range(0, len(list_model)):\n",
    "            proba_values.append(list_model[i].predict_proba(X_test))\n",
    "        return proba_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('emotion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "93535a4fe30750ce911c1007b9d15bfba5db07e1e17223c7001a65f5037bcf91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
